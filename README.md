# Class Project for MECH-501: HMMs for Object Classification

**Overview**: Object classification using microphone data gathered from a robotic hand manipulating different objects.

**Contents**
1. [Complete project notebook](main/main_notebook_3.ipynb): Jupyter notebook containing the experiments, results and report
  * All the contents/outputs of the notebook are viewable on GitHub
  * If the repository is downloaded, it is **not** advisable to run the entire notebook (some cells have hour-long runtimes)
  * Key results have been saved for visualization purposes and are available [here](main/search)
  * To try out the code, run all the cells with the note "RUN CELL" at the top
  * There is a section at the end of the notebook that can be run and demonstrates all the steps in the project pipeline (data processing, training, testing, visualization)
2. [Dataset](Toprak_Dataset): folder containing the raw data
  * Reference: S. Toprak, N. Navarro-Guerrero, and S. Wermter, “Evaluating Integration Strategies for Visuo-Haptic Object Recognition,” Cognitive Computation, vol. 10, no. 3, pp. 408–425, Dec. 2017, doi: https://doi.org/10.1007/s12559-017-9536-7.
  * Dataset available [here](https://figshare.com/articles/dataset/Supplementary_Material_for_Evaluating_Integration_Strategies_for_Visuo-Haptic_Object_Recognition_/5280949)


