# Class Project for MECH-501: HMMs for Object Classification

**Overview**: Object classification using microphone data gathered from a robotic hand manipulating different objects.

**Contents**
1. [Complete project notebook](main/main_notebook_3.ipynb): Jupyter notebook containing the experiments, results and report
  * All the contents/outputs of the notebook are viewable on GitHub; *this is the recommended way to view the results of all the experiments and read the report sections since it guarantees that everything will be correctly displayed*
  * There is a section at the end of the notebook that can be run and that demonstrates all the steps in the project pipeline (data processing, training, testing, visualization)
  * To run these cells, first run **all** the cells in the notebook with the note "#####RUN CELL#####" at the top
  * It is **not** advisable to run the entire notebook (some cells have hour-long runtimes)
  * Key results have been saved for visualization purposes and are available in the folder "search", however the main method for visualizing these results is through GitHub
2. The folder "Toprak_Dataset" contains the raw data
  * Reference: S. Toprak, N. Navarro-Guerrero, and S. Wermter, “Evaluating Integration Strategies for Visuo-Haptic Object Recognition,” Cognitive Computation, vol. 10, no. 3, pp. 408–425, Dec. 2017, doi: https://doi.org/10.1007/s12559-017-9536-7.
  * Dataset available [here](https://figshare.com/articles/dataset/Supplementary_Material_for_Evaluating_Integration_Strategies_for_Visuo-Haptic_Object_Recognition_/5280949)


